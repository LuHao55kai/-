def train(train_loader, model, criterion, optimizer, epoch):
        batch_time = AverageMeter()
        data_time = AverageMeter()
        losses = AverageMeter()
        acc = AverageMeter()

        # switch to train mode
        model.train()

        end = time.time()
        for i, (images, target) in enumerate(train_loader):
            # measure data loading
            # if len(target) % workers == 1:
            #     images = images[:-1]
            #     target = target[:-1]

            data_time.update(time.time() - end)
            image_var = torch.tensor(images, requires_grad=False).cuda(non_blocking=True)
            # print(image_var)
            label = torch.tensor(target).cuda(non_blocking=True)
            # compute y_pred
            y_pred = model(image_var)
            loss = criterion(y_pred, label)

            # measure accuracy and record loss
            prec, PRED_COUNT = accuracy(y_pred.data, target, topk=(1, 1))
            losses.update(loss.item(), images.size(0))
            acc.update(prec, PRED_COUNT)

            # compute gradient and do SGD step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if i % print_freq == 0:
                print('Epoch: [{0}][{1}/{2}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Accuray {acc.val:.3f} ({acc.avg:.3f})'.format(
                    epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses, acc=acc))
      
def test(test_loader, model):
        csv_map = OrderedDict({'FileName': [], 'type': [], 'probability': []})
        # switch to evaluate mode
        model.eval()
        for i, (images, filepath) in enumerate(tqdm(test_loader)):
            # bs, ncrops, c, h, w = images.size()

            filepath = [str(i) for i in filepath]
            image_var = torch.tensor(images, requires_grad=False)  # for pytorch 0.4

            with torch.no_grad():
                y_pred = model(image_var)  # fuse batch size and ncrops
                # y_pred = y_pred.view(bs, ncrops, -1).mean(1) # avg over crops

                # get the index of the max log-probability
                smax = nn.Softmax()
                smax_out = smax(y_pred)
            csv_map['FileName'].extend(filepath)
            for output in smax_out:
                prob = ';'.join([str(i) for i in output.data.tolist()])
                csv_map['probability'].append(prob)
                csv_map['type'].append(np.argmax(output.data.tolist()))
            # print(len(csv_map['filename']), len(csv_map['probability']))

        result = pd.DataFrame(csv_map)
        result.to_csv('./result/%s/submission.csv' % file_name, index=False)
        result[['FileName','type']].to_csv('./result/%s/final_submission.csv' % file_name, index=False)
        return
